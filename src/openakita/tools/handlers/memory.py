"""
è®°å¿†ç³»ç»Ÿå¤„ç†å™¨

å¤„ç†è®°å¿†ç›¸å…³çš„ç³»ç»ŸæŠ€èƒ½ï¼š
- add_memory: æ·»åŠ è®°å¿†
- search_memory: æœç´¢è®°å¿†
- get_memory_stats: è·å–è®°å¿†ç»Ÿè®¡
- list_recent_tasks: åˆ—å‡ºæœ€è¿‘ä»»åŠ¡
- search_conversation_traces: æœç´¢å®Œæ•´å¯¹è¯å†å²
- trace_memory: è·¨å±‚å¯¼èˆªï¼ˆè®°å¿†â†”æƒ…èŠ‚â†”å¯¹è¯ï¼‰
"""

import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from ...core.agent import Agent

logger = logging.getLogger(__name__)


class MemoryHandler:
    """
    è®°å¿†ç³»ç»Ÿå¤„ç†å™¨

    å¤„ç†æ‰€æœ‰è®°å¿†ç›¸å…³çš„å·¥å…·è°ƒç”¨
    """

    TOOLS = [
        "consolidate_memories",
        "add_memory",
        "search_memory",
        "get_memory_stats",
        "list_recent_tasks",
        "search_conversation_traces",
        "trace_memory",
    ]

    _SEARCH_TOOLS = frozenset({
        "search_memory", "list_recent_tasks", "trace_memory", "search_conversation_traces",
    })

    _NAVIGATION_GUIDE = (
        "ğŸ“– è®°å¿†ç³»ç»Ÿå¯¼èˆªæŒ‡å—ï¼ˆä»…æ˜¾ç¤ºä¸€æ¬¡ï¼‰\n\n"
        "## ä¸‰å±‚å…³è”æœºåˆ¶\n"
        "- è®°å¿† â†’ æƒ…èŠ‚ï¼šæ¯æ¡è®°å¿†æœ‰ source_episode_idï¼ŒæŒ‡å‘äº§ç”Ÿå®ƒçš„ä»»åŠ¡æƒ…èŠ‚\n"
        "- æƒ…èŠ‚ â†’ è®°å¿†ï¼šæ¯ä¸ªæƒ…èŠ‚æœ‰ linked_memory_idsï¼Œåˆ—å‡ºå®ƒäº§å‡ºçš„è®°å¿†\n"
        "- æƒ…èŠ‚ â†’ å¯¹è¯ï¼šé€šè¿‡ session_id å…³è”åˆ°åŸå§‹å¯¹è¯è½®æ¬¡\n\n"
        "## å·¥å…·è¯¦è§£\n"
        "- search_memory â€” æœç´¢æç‚¼åçš„çŸ¥è¯†ï¼ˆåå¥½/è§„åˆ™/ç»éªŒ/æŠ€èƒ½ï¼‰ï¼Œç»“æœå«æ¥æºæƒ…èŠ‚ ID\n"
        "- list_recent_tasks â€” åˆ—å‡ºæœ€è¿‘ä»»åŠ¡æƒ…èŠ‚ï¼Œå«å…³è”è®°å¿†æ•°å’Œå·¥å…·åˆ—è¡¨\n"
        "- trace_memory â€” è·¨å±‚å¯¼èˆªç”µæ¢¯ï¼š\n"
        "  Â· ä¼  memory_id â†’ è¿”å›æºæƒ…èŠ‚æ‘˜è¦ + ç›¸å…³å¯¹è¯ç‰‡æ®µ\n"
        "  Â· ä¼  episode_id â†’ è¿”å›å…³è”è®°å¿†åˆ—è¡¨ + å¯¹è¯åŸæ–‡\n"
        "- search_conversation_traces â€” åŸå§‹å¯¹è¯å…¨æ–‡æœç´¢ï¼ˆå‚æ•°+è¿”å›å€¼ï¼‰\n"
        "- add_memory â€” ä¸»åŠ¨è®°å½•ç»éªŒ(experience/skill)ã€æ•™è®­(error)ã€åå¥½(preference/rule)\n\n"
        "## æœç´¢ç­–ç•¥ï¼šå…ˆæ¦‚è§ˆï¼Œå†æ·±å…¥\n"
        "1. search_memory æŸ¥ç°æˆçš„ç»éªŒ/è§„åˆ™/äº‹å®\n"
        "2. éœ€è¦ä¸Šä¸‹æ–‡ â†’ trace_memory(memory_id=...) æº¯æºåˆ°æƒ…èŠ‚å’Œå¯¹è¯\n"
        "3. å¯¹æŸä¸ªæƒ…èŠ‚æ„Ÿå…´è¶£ â†’ trace_memory(episode_id=...) æŸ¥å…³è”è®°å¿†å’Œå¯¹è¯\n"
        "4. ä»¥ä¸Šéƒ½æ²¡ç»“æœ â†’ search_conversation_traces å…¨æ–‡æœç´¢\n\n"
        "## ä½•æ—¶æœç´¢\n"
        "- ç”¨æˆ·é—®\"åšäº†ä»€ä¹ˆ\" â†’ list_recent_tasks\n"
        "- ç”¨æˆ·æåˆ°\"ä¹‹å‰/ä¸Šæ¬¡\" â†’ search_memory\n"
        "- éœ€è¦æ“ä½œç»†èŠ‚/å…·ä½“å‘½ä»¤ â†’ trace_memory æˆ– search_conversation_traces\n"
        "- åšè¿‡ç±»ä¼¼ä»»åŠ¡ â†’ å…ˆ search_memory æŸ¥ç»éªŒï¼Œéœ€è¦ç»†èŠ‚å† trace_memory\n"
        "- ä¸ç¡®å®šæ—¶ â†’ ä¸æœç´¢\n\n"
        "---\n\n"
    )

    def __init__(self, agent: "Agent"):
        self.agent = agent
        self._guide_injected: bool = False

    def reset_guide(self) -> None:
        """Reset the one-shot guide flag (call on new session start)."""
        self._guide_injected = False

    async def handle(self, tool_name: str, params: dict[str, Any]) -> str:
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        if tool_name == "consolidate_memories":
            return await self._consolidate_memories(params)
        elif tool_name == "add_memory":
            return self._add_memory(params)
        elif tool_name == "search_memory":
            result = self._search_memory(params)
        elif tool_name == "get_memory_stats":
            return self._get_memory_stats(params)
        elif tool_name == "list_recent_tasks":
            result = self._list_recent_tasks(params)
        elif tool_name == "search_conversation_traces":
            result = self._search_conversation_traces(params)
        elif tool_name == "trace_memory":
            result = self._trace_memory(params)
        else:
            return f"âŒ Unknown memory tool: {tool_name}"

        if tool_name in self._SEARCH_TOOLS and not self._guide_injected:
            self._guide_injected = True
            return self._NAVIGATION_GUIDE + result
        return result

    async def _consolidate_memories(self, params: dict) -> str:
        """æ‰‹åŠ¨è§¦å‘è®°å¿†æ•´ç†"""
        try:
            from ...config import settings
            from ...scheduler.consolidation_tracker import ConsolidationTracker

            tracker = ConsolidationTracker(settings.project_root / "data" / "scheduler")
            since, until = tracker.get_memory_consolidation_time_range()

            result = await self.agent.memory_manager.consolidate_daily()

            tracker.record_memory_consolidation(result)

            time_range = (
                f"{since.strftime('%m-%d %H:%M')} â†’ {until.strftime('%m-%d %H:%M')}"
                if since else "å…¨éƒ¨è®°å½•"
            )

            lines = ["âœ… è®°å¿†æ•´ç†å®Œæˆ:"]
            if result.get("unextracted_processed"):
                lines.append(f"- æ–°æå–: {result['unextracted_processed']} æ¡")
            if result.get("duplicates_removed"):
                lines.append(f"- å»é‡: {result['duplicates_removed']} æ¡")
            if result.get("memories_decayed"):
                lines.append(f"- è¡°å‡æ¸…ç†: {result['memories_decayed']} æ¡")

            review = result.get("llm_review", {})
            if review.get("deleted") or review.get("updated") or review.get("merged"):
                lines.append(f"- LLM å®¡æŸ¥: åˆ é™¤ {review.get('deleted', 0)}, "
                             f"æ›´æ–° {review.get('updated', 0)}, "
                             f"åˆå¹¶ {review.get('merged', 0)}, "
                             f"ä¿ç•™ {review.get('kept', 0)}")

            if result.get("sessions_processed"):
                lines.append(f"- å¤„ç†ä¼šè¯: {result['sessions_processed']}")
            lines.append(f"- æ—¶é—´èŒƒå›´: {time_range}")
            return "\n".join(lines)

        except Exception as e:
            logger.error(f"Manual memory consolidation failed: {e}", exc_info=True)
            return f"âŒ è®°å¿†æ•´ç†å¤±è´¥: {e}"

    def _add_memory(self, params: dict) -> str:
        """æ·»åŠ è®°å¿†"""
        from ...memory.types import Memory, MemoryPriority, MemoryType

        content = params["content"]
        mem_type_str = params["type"]
        importance = params.get("importance", 0.5)

        type_map = {
            "fact": MemoryType.FACT,
            "preference": MemoryType.PREFERENCE,
            "skill": MemoryType.SKILL,
            "error": MemoryType.ERROR,
            "rule": MemoryType.RULE,
        }
        mem_type = type_map.get(mem_type_str, MemoryType.FACT)

        if importance >= 0.8:
            priority = MemoryPriority.PERMANENT
        elif importance >= 0.6:
            priority = MemoryPriority.LONG_TERM
        else:
            priority = MemoryPriority.SHORT_TERM

        memory = Memory(
            type=mem_type,
            priority=priority,
            content=content,
            source="manual",
            importance_score=importance,
        )

        memory_id = self.agent.memory_manager.add_memory(memory)
        if memory_id:
            return f"âœ… å·²è®°ä½: [{mem_type_str}] {content}\nID: {memory_id}"
        else:
            return "âœ… è®°å¿†å·²å­˜åœ¨ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰ï¼Œæ— éœ€é‡å¤è®°å½•ã€‚è¯·ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡æˆ–ç»“æŸã€‚"

    def _search_memory(self, params: dict) -> str:
        """æœç´¢è®°å¿†

        æ—  type_filter: RetrievalEngine å¤šè·¯å¬å›ï¼ˆè¯­ä¹‰+æƒ…èŠ‚+æœ€è¿‘+é™„ä»¶ï¼‰
        æœ‰ type_filter: SQLite FTS5 æœç´¢ + ç±»å‹è¿‡æ»¤
        æœ€ç»ˆ fallback: v1 å†…å­˜å­ä¸²åŒ¹é…
        """
        from ...memory.types import MemoryType

        query = params["query"]
        type_filter = params.get("type")
        now = datetime.now()

        mm = self.agent.memory_manager

        # è·¯å¾„ A: æ— ç±»å‹è¿‡æ»¤ â†’ RetrievalEngine å¤šè·¯å¬å›
        if not type_filter:
            retrieval_engine = getattr(mm, "retrieval_engine", None)
            if retrieval_engine:
                try:
                    candidates = retrieval_engine.retrieve_candidates(
                        query=query,
                        recent_messages=getattr(mm, "_recent_messages", None),
                    )
                    if candidates:
                        logger.info(f"[search_memory] RetrievalEngine: {len(candidates)} candidates for '{query[:50]}'")
                        cited = [{"id": c.memory_id, "content": c.content[:200]} for c in candidates[:10] if c.memory_id]
                        if cited:
                            mm.record_cited_memories(cited)
                        output = f"æ‰¾åˆ° {len(candidates)} æ¡ç›¸å…³è®°å¿†:\n\n"
                        for c in candidates[:10]:
                            ep_hint = ""
                            if hasattr(c, "episode_id") and c.episode_id:
                                ep_hint = f", æ¥æºæƒ…èŠ‚: {c.episode_id[:12]}"
                            output += f"- [{c.source_type}] {c.content[:200]}{ep_hint}\n\n"
                        return output
                except Exception as e:
                    logger.warning(f"[search_memory] RetrievalEngine failed: {e}")

        # è·¯å¾„ B: æœ‰ç±»å‹è¿‡æ»¤ æˆ– RetrievalEngine æ— ç»“æœ â†’ SQLite æœç´¢
        store = getattr(mm, "store", None)
        if store:
            try:
                memories = store.search_semantic(query, limit=10, filter_type=type_filter)
                memories = [m for m in memories if not m.expires_at or m.expires_at >= now]
                if memories:
                    logger.info(f"[search_memory] SQLite: {len(memories)} results for '{query[:50]}'")
                    cited = [{"id": m.id, "content": m.content[:200]} for m in memories]
                    mm.record_cited_memories(cited)
                    output = f"æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†:\n\n"
                    for m in memories:
                        ep_hint = f", æ¥æºæƒ…èŠ‚: {m.source_episode_id[:12]}" if m.source_episode_id else ""
                        output += f"- [{m.type.value}] {m.content}\n"
                        output += f"  (é‡è¦æ€§: {m.importance_score:.1f}, å¼•ç”¨: {m.access_count}{ep_hint})\n\n"
                    return output
            except Exception as e:
                logger.warning(f"[search_memory] SQLite search failed: {e}")

        # è·¯å¾„ C: æœ€ç»ˆ fallback â†’ v1 å†…å­˜å­ä¸²åŒ¹é…
        mem_type = None
        if type_filter:
            type_map = {
                "fact": MemoryType.FACT,
                "preference": MemoryType.PREFERENCE,
                "skill": MemoryType.SKILL,
                "error": MemoryType.ERROR,
                "rule": MemoryType.RULE,
                "experience": MemoryType.EXPERIENCE,
            }
            mem_type = type_map.get(type_filter)

        memories = mm.search_memories(
            query=query, memory_type=mem_type, limit=10
        )
        memories = [m for m in memories if not m.expires_at or m.expires_at >= now]

        if not memories:
            return f"æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„è®°å¿†"

        cited = [{"id": m.id, "content": m.content[:200]} for m in memories]
        mm.record_cited_memories(cited)

        output = f"æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†:\n\n"
        for m in memories:
            ep_hint = f", æ¥æºæƒ…èŠ‚: {m.source_episode_id[:12]}" if m.source_episode_id else ""
            output += f"- [{m.type.value}] {m.content}\n"
            output += f"  (é‡è¦æ€§: {m.importance_score:.1f}, å¼•ç”¨: {m.access_count}{ep_hint})\n\n"

        return output

    def _get_memory_stats(self, params: dict) -> str:
        """è·å–è®°å¿†ç»Ÿè®¡"""
        stats = self.agent.memory_manager.get_stats()

        output = f"""è®°å¿†ç³»ç»Ÿç»Ÿè®¡:

- æ€»è®°å¿†æ•°: {stats["total"]}
- ä»Šæ—¥ä¼šè¯: {stats["sessions_today"]}
- å¾…å¤„ç†ä¼šè¯: {stats["unprocessed_sessions"]}

æŒ‰ç±»å‹:
"""
        for type_name, count in stats.get("by_type", {}).items():
            output += f"  - {type_name}: {count}\n"

        output += "\næŒ‰ä¼˜å…ˆçº§:\n"
        for priority, count in stats.get("by_priority", {}).items():
            output += f"  - {priority}: {count}\n"

        return output


    def _list_recent_tasks(self, params: dict) -> str:
        """åˆ—å‡ºæœ€è¿‘å®Œæˆçš„ä»»åŠ¡ï¼ˆEpisodeï¼‰"""
        days = params.get("days", 3)
        limit = params.get("limit", 15)

        mm = self.agent.memory_manager
        store = getattr(mm, "store", None)
        if not store:
            return "è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–"

        episodes = store.get_recent_episodes(days=days, limit=limit)
        if not episodes:
            return f"æœ€è¿‘ {days} å¤©æ²¡æœ‰å·²å®Œæˆçš„ä»»åŠ¡è®°å½•ã€‚"

        lines = [f"æœ€è¿‘ {days} å¤©å®Œæˆçš„ä»»åŠ¡ï¼ˆå…± {len(episodes)} æ¡ï¼‰ï¼š\n"]
        for i, ep in enumerate(episodes, 1):
            goal = ep.goal or "(æœªè®°å½•ç›®æ ‡)"
            outcome = ep.outcome or "completed"
            tools = ", ".join(ep.tools_used[:5]) if ep.tools_used else "æ— å·¥å…·è°ƒç”¨"
            sa = ep.started_at
            started = sa.strftime("%Y-%m-%d %H:%M") if hasattr(sa, "strftime") else str(sa)[:16]
            mem_count = len(ep.linked_memory_ids) if ep.linked_memory_ids else 0
            lines.append(f"{i}. [{started}] {goal}  (id: {ep.id[:12]})")
            mem_hint = f"å…³è”è®°å¿†: {mem_count}æ¡ | " if mem_count else ""
            lines.append(f"   ç»“æœ: {outcome} | {mem_hint}å·¥å…·: {tools}")
            if ep.summary:
                lines.append(f"   æ‘˜è¦: {ep.summary[:120]}")
            lines.append("")

        return "\n".join(lines)

    def _search_conversation_traces(self, params: dict) -> str:
        """æœç´¢å®Œæ•´å¯¹è¯å†å²ï¼ˆå«å·¥å…·è°ƒç”¨å’Œç»“æœï¼‰

        ä¼˜å…ˆä» SQLite conversation_turns æœç´¢ï¼ˆå¯é ã€æœ‰ç´¢å¼•ï¼‰ï¼Œ
        ä¸è¶³æ—¶å† fallback åˆ° JSONL æ–‡ä»¶å’Œ react_tracesã€‚
        """
        keyword = params.get("keyword", "").strip()
        if not keyword:
            return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"

        session_id_filter = params.get("session_id", "")
        max_results = params.get("max_results", 10)
        days_back = params.get("days_back", 7)

        logger.info(
            f"[SearchTraces] keyword={keyword!r}, session={session_id_filter!r}, "
            f"max={max_results}, days_back={days_back}"
        )

        results: list[dict] = []

        # === æ•°æ®æº 1: SQLite conversation_turnsï¼ˆä¸»æ•°æ®æºï¼‰ ===
        store = getattr(self.agent.memory_manager, "store", None)
        if store:
            try:
                rows = store.search_turns(
                    keyword=keyword,
                    session_id=session_id_filter or None,
                    days_back=days_back,
                    limit=max_results,
                )
                for row in rows:
                    results.append({
                        "source": "sqlite_turns",
                        "session_id": row.get("session_id", ""),
                        "episode_id": row.get("episode_id", ""),
                        "timestamp": row.get("timestamp", ""),
                        "role": row.get("role", ""),
                        "content": str(row.get("content", ""))[:500],
                        "tool_calls": row.get("tool_calls") or [],
                        "tool_results": row.get("tool_results") or [],
                    })
            except Exception as e:
                logger.warning(f"[SearchTraces] SQLite search failed, will try JSONL: {e}")

        # === æ•°æ®æº 2: react_tracesï¼ˆè¡¥å……å·¥å…·è°ƒç”¨ç»†èŠ‚ï¼‰ ===
        if len(results) < max_results:
            cutoff = datetime.now() - timedelta(days=days_back)
            from ...config import settings
            data_root = settings.project_root / "data"

            traces_dir = data_root / "react_traces"
            if traces_dir.exists():
                remaining = max_results - len(results)
                seen_timestamps = {r.get("timestamp", "") for r in results}
                self._search_react_traces(
                    traces_dir, keyword, session_id_filter, cutoff, remaining,
                    results, seen_timestamps,
                )

        # === æ•°æ®æº 3: JSONL fallbackï¼ˆSQLite æ— ç»“æœæˆ–æ›´æ—©å†å²ï¼‰ ===
        if len(results) < max_results:
            cutoff = datetime.now() - timedelta(days=days_back)
            from ...config import settings
            data_root = settings.project_root / "data"

            history_dir = data_root / "memory" / "conversation_history"
            if history_dir.exists():
                remaining = max_results - len(results)
                seen_timestamps = {r.get("timestamp", "") for r in results}
                self._search_jsonl_history(
                    history_dir, keyword, session_id_filter, cutoff, remaining,
                    results, seen_timestamps,
                )

        if not results:
            return f"æœªæ‰¾åˆ°åŒ…å« '{keyword}' çš„å¯¹è¯è®°å½•ï¼ˆæœ€è¿‘ {days_back} å¤©ï¼‰"

        return self._format_trace_results(results, keyword)

    def _trace_memory(self, params: dict) -> str:
        """è·¨å±‚å¯¼èˆªï¼šä»è®°å¿†â†’æƒ…èŠ‚â†’å¯¹è¯ï¼Œæˆ–ä»æƒ…èŠ‚â†’è®°å¿†+å¯¹è¯"""
        memory_id = params.get("memory_id", "").strip()
        episode_id = params.get("episode_id", "").strip()

        if not memory_id and not episode_id:
            return "è¯·æä¾› memory_id æˆ– episode_id å…¶ä¸­ä¸€ä¸ª"

        mm = self.agent.memory_manager
        store = getattr(mm, "store", None)
        if not store:
            return "è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–"

        if memory_id:
            return self._trace_from_memory(store, memory_id)
        else:
            return self._trace_from_episode(store, episode_id)

    def _trace_from_memory(self, store, memory_id: str) -> str:
        """memory_id â†’ source episode â†’ conversation turns"""
        mem = store.get_semantic(memory_id)
        if not mem:
            return f"æœªæ‰¾åˆ°è®°å¿† {memory_id}"

        lines = ["## è®°å¿†è¯¦æƒ…\n"]
        lines.append(f"- [{mem.type.value}] {mem.content}")
        lines.append(f"  é‡è¦æ€§: {mem.importance_score:.1f}, å¼•ç”¨: {mem.access_count}, ç½®ä¿¡åº¦: {mem.confidence:.1f}")

        ep_id = mem.source_episode_id
        if not ep_id:
            lines.append("\nè¯¥è®°å¿†æ²¡æœ‰å…³è”æƒ…èŠ‚ï¼ˆå¯èƒ½æ˜¯æ‰‹åŠ¨æ·»åŠ æˆ–æ—©æœŸæå–çš„ï¼‰ã€‚")
            return "\n".join(lines)

        ep = store.get_episode(ep_id)
        if not ep:
            lines.append(f"\nå…³è”æƒ…èŠ‚ {ep_id} å·²ä¸å­˜åœ¨ã€‚")
            return "\n".join(lines)

        lines.append("\n## æ¥æºæƒ…èŠ‚\n")
        lines.append(f"- ç›®æ ‡: {ep.goal or '(æœªè®°å½•)'}")
        lines.append(f"- ç»“æœ: {ep.outcome}")
        lines.append(f"- æ‘˜è¦: {ep.summary[:200]}")
        sa = ep.started_at
        started = sa.strftime("%Y-%m-%d %H:%M") if hasattr(sa, "strftime") else str(sa)[:16]
        lines.append(f"- æ—¶é—´: {started}")
        if ep.tools_used:
            lines.append(f"- å·¥å…·: {', '.join(ep.tools_used[:8])}")

        turns = store.get_session_turns(ep.session_id)
        if turns:
            lines.append(f"\n## ç›¸å…³å¯¹è¯ï¼ˆå…± {len(turns)} è½®ï¼Œæ˜¾ç¤ºå‰ 6 è½®ï¼‰\n")
            for t in turns[:6]:
                role = t.get("role", "?")
                content = str(t.get("content", ""))[:200]
                lines.append(f"[{role}] {content}")
                if t.get("tool_calls"):
                    tc = t["tool_calls"]
                    if isinstance(tc, list):
                        names = [c.get("name", "?") for c in tc if isinstance(c, dict)]
                        if names:
                            lines.append(f"  â†’ å·¥å…·è°ƒç”¨: {', '.join(names)}")
                lines.append("")

        return "\n".join(lines)

    def _trace_from_episode(self, store, episode_id: str) -> str:
        """episode_id â†’ linked memories + conversation turns"""
        ep = store.get_episode(episode_id)
        if not ep:
            return f"æœªæ‰¾åˆ°æƒ…èŠ‚ {episode_id}"

        lines = ["## æƒ…èŠ‚è¯¦æƒ…\n"]
        lines.append(f"- ç›®æ ‡: {ep.goal or '(æœªè®°å½•)'}")
        lines.append(f"- ç»“æœ: {ep.outcome}")
        lines.append(f"- æ‘˜è¦: {ep.summary[:200]}")
        sa = ep.started_at
        started = sa.strftime("%Y-%m-%d %H:%M") if hasattr(sa, "strftime") else str(sa)[:16]
        lines.append(f"- æ—¶é—´: {started}")
        if ep.tools_used:
            lines.append(f"- å·¥å…·: {', '.join(ep.tools_used[:8])}")

        if ep.linked_memory_ids:
            lines.append(f"\n## å…³è”è®°å¿†ï¼ˆ{len(ep.linked_memory_ids)} æ¡ï¼‰\n")
            for mid in ep.linked_memory_ids[:10]:
                mem = store.get_semantic(mid)
                if mem:
                    lines.append(f"- [{mem.type.value}] {mem.content[:150]}")
                else:
                    lines.append(f"- (å·²åˆ é™¤) {mid[:12]}")
        else:
            lines.append("\nè¯¥æƒ…èŠ‚å°šæ— å…³è”è®°å¿†ã€‚")

        turns = store.get_session_turns(ep.session_id)
        if turns:
            lines.append(f"\n## å¯¹è¯åŸæ–‡ï¼ˆå…± {len(turns)} è½®ï¼Œæ˜¾ç¤ºå‰ 8 è½®ï¼‰\n")
            for t in turns[:8]:
                role = t.get("role", "?")
                content = str(t.get("content", ""))[:300]
                lines.append(f"[{role}] {content}")
                if t.get("tool_calls"):
                    tc = t["tool_calls"]
                    if isinstance(tc, list):
                        for c in tc[:3]:
                            if isinstance(c, dict):
                                lines.append(f"  â†’ {c.get('name', '?')}: {json.dumps(c.get('input', {}), ensure_ascii=False, default=str)[:200]}")
                lines.append("")

        return "\n".join(lines)

    def _search_react_traces(
        self,
        traces_dir: Path,
        keyword: str,
        session_id_filter: str,
        cutoff: datetime,
        limit: int,
        results: list[dict],
        seen_timestamps: set[str],
    ) -> None:
        """æœç´¢ react_traces/{date}/*.json"""
        count = 0
        for date_dir in sorted(traces_dir.iterdir(), reverse=True):
            if not date_dir.is_dir():
                continue
            try:
                dir_date = datetime.strptime(date_dir.name, "%Y%m%d")
                if dir_date < cutoff:
                    continue
            except ValueError:
                continue
            for trace_file in sorted(date_dir.glob("*.json"), reverse=True):
                if session_id_filter and session_id_filter not in trace_file.stem:
                    continue
                try:
                    raw = trace_file.read_text(encoding="utf-8")
                    if keyword.lower() not in raw.lower():
                        continue
                    trace_data = json.loads(raw)
                except Exception:
                    continue
                for it in trace_data.get("iterations", []):
                    it_str = json.dumps(it, ensure_ascii=False, default=str)
                    if keyword.lower() not in it_str.lower():
                        continue
                    results.append({
                        "source": "react_trace",
                        "file": f"{date_dir.name}/{trace_file.name}",
                        "conversation_id": trace_data.get("conversation_id", ""),
                        "iteration": it.get("iteration", 0),
                        "tool_calls": it.get("tool_calls", []),
                        "tool_results": it.get("tool_results", []),
                        "text_content": str(it.get("text_content", ""))[:300],
                    })
                    count += 1
                    if count >= limit:
                        return
                if count >= limit:
                    return
            if count >= limit:
                return

    def _search_jsonl_history(
        self,
        history_dir: Path,
        keyword: str,
        session_id_filter: str,
        cutoff: datetime,
        limit: int,
        results: list[dict],
        seen_timestamps: set[str],
    ) -> None:
        """æœç´¢ conversation_history/*.jsonlï¼Œè·³è¿‡ SQLite å·²è¿”å›çš„æ¡ç›®"""
        count = 0
        for jsonl_file in sorted(history_dir.glob("*.jsonl"), reverse=True):
            if session_id_filter and session_id_filter not in jsonl_file.stem:
                continue
            try:
                file_mtime = datetime.fromtimestamp(jsonl_file.stat().st_mtime)
                if file_mtime < cutoff:
                    continue
            except Exception:
                continue
            try:
                for line in jsonl_file.read_text(encoding="utf-8").splitlines():
                    if not line.strip():
                        continue
                    if keyword.lower() not in line.lower():
                        continue
                    try:
                        turn = json.loads(line)
                    except json.JSONDecodeError:
                        continue
                    ts = turn.get("timestamp", "")
                    if ts in seen_timestamps:
                        continue
                    results.append({
                        "source": "conversation_history",
                        "file": jsonl_file.name,
                        "timestamp": ts,
                        "role": turn.get("role", ""),
                        "content": str(turn.get("content", ""))[:500],
                        "tool_calls": turn.get("tool_calls", []),
                        "tool_results": turn.get("tool_results", []),
                    })
                    seen_timestamps.add(ts)
                    count += 1
                    if count >= limit:
                        return
            except Exception as e:
                logger.debug(f"Error reading {jsonl_file}: {e}")
            if count >= limit:
                return

    @staticmethod
    def _format_trace_results(results: list[dict], keyword: str) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºå¯è¯»æ–‡æœ¬"""
        output = f"æ‰¾åˆ° {len(results)} æ¡åŒ¹é…è®°å½•ï¼ˆå…³é”®è¯: {keyword}ï¼‰:\n\n"
        for i, r in enumerate(results, 1):
            source = r["source"]
            output += f"--- è®°å½• {i} [{source}] ---\n"
            if source in ("sqlite_turns", "conversation_history"):
                if r.get("session_id"):
                    output += f"ä¼šè¯: {r['session_id']}\n"
                elif r.get("file"):
                    output += f"æ–‡ä»¶: {r['file']}\n"
                if r.get("episode_id"):
                    output += f"å…³è”æƒ…èŠ‚: {r['episode_id'][:12]}\n"
                output += f"æ—¶é—´: {r.get('timestamp', 'N/A')}\n"
                output += f"è§’è‰²: {r.get('role', 'N/A')}\n"
                output += f"å†…å®¹: {r.get('content', '')}\n"
                if r.get("tool_calls"):
                    output += f"å·¥å…·è°ƒç”¨: {json.dumps(r['tool_calls'], ensure_ascii=False, default=str)[:500]}\n"
                if r.get("tool_results"):
                    output += f"å·¥å…·ç»“æœ: {json.dumps(r['tool_results'], ensure_ascii=False, default=str)[:500]}\n"
            else:
                output += f"æ–‡ä»¶: {r.get('file', 'N/A')}\n"
                output += f"ä¼šè¯: {r.get('conversation_id', 'N/A')}\n"
                output += f"è¿­ä»£: {r.get('iteration', 'N/A')}\n"
                if r.get("text_content"):
                    output += f"æ–‡æœ¬: {r['text_content']}\n"
                if r.get("tool_calls"):
                    for tc in r["tool_calls"]:
                        output += f"  å·¥å…·: {tc.get('name', 'N/A')}\n"
                        inp = tc.get("input", {})
                        if isinstance(inp, dict):
                            inp_str = json.dumps(inp, ensure_ascii=False, default=str)
                            output += f"  å‚æ•°: {inp_str[:300]}\n"
                if r.get("tool_results"):
                    for tr in r["tool_results"]:
                        rc = str(tr.get("result_content", tr.get("result_preview", "")))
                        output += f"  ç»“æœ: {rc[:300]}\n"
            output += "\n"
        return output


def create_handler(agent: "Agent"):
    """åˆ›å»ºè®°å¿†å¤„ç†å™¨"""
    handler = MemoryHandler(agent)
    agent._memory_handler = handler
    return handler.handle
